{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ks2aVpiusZx"
      },
      "outputs": [],
      "source": [
        "!pip install arch\n",
        "pip install scikeras\n",
        "pip install keras-tuner\n",
        "!pip install yfinance\n",
        "pip install statsmodels scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7kWjEhZnWXz"
      },
      "outputs": [],
      "source": [
        "import platform\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "import kerastuner as kt\n",
        "from tensorflow.keras.layers import InputLayer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F55dshn8F23i"
      },
      "outputs": [],
      "source": [
        "# Define ticker symbols and date range\n",
        "ticker_symbols = [\n",
        "    'AMG', 'AMZN', 'AZO', 'BKNG', 'BLK', 'DUK', 'ED', 'F', 'FCX', 'GE',\n",
        "    'GOOG', 'GPS', 'HBAN', 'ICE', 'LDOS', 'LMT', 'M', 'MTD', 'MYL', 'NKE',\n",
        "    'ORLY', 'PBCT', 'PLD', 'RF', 'RJF', 'SHW', 'TDG', 'UAA', 'VFC', 'WEC'\n",
        "]\n",
        "start_date = '2010-01-01'\n",
        "end_date = '2019-09-01'\n",
        "\n",
        "# Fetch and prepare data\n",
        "all_data = pd.DataFrame()\n",
        "for symbol in ticker_symbols:\n",
        "    data = yf.download(symbol, start=start_date, end=end_date)\n",
        "    data['Ticker'] = symbol\n",
        "    all_data = pd.concat([all_data, data])\n",
        "\n",
        "all_data.index = pd.to_datetime(all_data.index)\n",
        "\n",
        "# Count unique trading days\n",
        "unique_trading_days = all_data.index.unique()\n",
        "number_of_trading_days = len(unique_trading_days)\n",
        "print(f\"Number of unique trading days: {number_of_trading_days}\")\n",
        "\n",
        "feature_names = all_data.columns.tolist()\n",
        "print(\"Feature names in the dataset:\", feature_names)\n",
        "\n",
        "all_data.ffill(inplace=True)\n",
        "all_data.bfill(inplace=True)\n",
        "\n",
        "all_data.to_csv(\"stock_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlusU2w0su5g"
      },
      "outputs": [],
      "source": [
        "# Relative Strength Index (RSI)\n",
        "def RSI(series, period=14):\n",
        "    delta = series.diff(1)\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
        "    RS = gain / loss\n",
        "    return 100 - (100 / (1 + RS))\n",
        "\n",
        "all_data['RSI'] = RSI(all_data['Adj Close'])\n",
        "\n",
        "# Moving Average Convergence Divergence (MACD)\n",
        "def MACD(series, fast=12, slow=26, signal=9):\n",
        "    exp1 = series.ewm(span=fast, adjust=False).mean()\n",
        "    exp2 = series.ewm(span=slow, adjust=False).mean()\n",
        "    macd = exp1 - exp2\n",
        "    signal_line = macd.ewm(span=signal, adjust=False).mean()\n",
        "    return macd, signal_line, macd - signal_line\n",
        "\n",
        "all_data['MACD'], all_data['MACDSignal'], all_data['MACDHist'] = MACD(all_data['Adj Close'])\n",
        "\n",
        "# Simple Moving Average (SMA)\n",
        "all_data['SMA'] = all_data['Adj Close'].rolling(window=10).mean()\n",
        "\n",
        "# Exponential Moving Average (EMA)\n",
        "all_data['EMA'] = all_data['Adj Close'].ewm(span=10, adjust=False).mean()\n",
        "\n",
        "# Average True Range (ATR)\n",
        "def ATR(df, period=14):\n",
        "    high_low = df['High'] - df['Low']\n",
        "    high_close = np.abs(df['High'] - df['Close'].shift())\n",
        "    low_close = np.abs(df['Low'] - df['Close'].shift())\n",
        "    ranges = pd.concat([high_low, high_close, low_close], axis=1)\n",
        "    true_range = np.max(ranges, axis=1)\n",
        "    atr = true_range.rolling(period).mean()\n",
        "    return atr\n",
        "\n",
        "all_data['ATR'] = ATR(all_data)\n",
        "\n",
        "# Bollinger Bands\n",
        "def bollinger_bands(series, period=20, std_dev=2):\n",
        "    sma = series.rolling(window=period).mean()\n",
        "    rstd = series.rolling(window=period).std()\n",
        "    upper_band = sma + std_dev * rstd\n",
        "    lower_band = sma - std_dev * rstd\n",
        "    return upper_band, sma, lower_band\n",
        "\n",
        "all_data['UpperBB'], all_data['MiddleBB'], all_data['LowerBB'] = bollinger_bands(all_data['Adj Close'])\n",
        "\n",
        "# Calculate Log Returns\n",
        "all_data['Log_Returns'] = np.log(all_data['Adj Close'] / all_data['Adj Close'].shift(1))\n",
        "\n",
        "# Date-Based Indicators\n",
        "all_data['day_of_week'] = all_data.index.dayofweek\n",
        "all_data['month'] = all_data.index.month\n",
        "all_data['quarter'] = all_data.index.quarter\n",
        "\n",
        "# Moving averages over different windows to capture trends\n",
        "all_data['rolling_mean_30'] = all_data['Adj Close'].rolling(window=30).mean()\n",
        "all_data['rolling_mean_90'] = all_data['Adj Close'].rolling(window=90).mean()\n",
        "\n",
        "# Rolling standard deviation to capture volatility changes\n",
        "all_data['rolling_std_30'] = all_data['Adj Close'].rolling(window=30).std()\n",
        "\n",
        "# Fourier series terms for seasonality\n",
        "def fourier_series(length, periods):\n",
        "    return np.column_stack([\n",
        "        np.cos(2 * np.pi * np.arange(length) / p) for p in periods] +\n",
        "        [np.sin(2 * np.pi * np.arange(length) / p) for p in periods])\n",
        "\n",
        "periods = [365.25/2, 365.25, 365.25*2]\n",
        "fourier_terms = fourier_series(len(all_data), periods)\n",
        "fourier_df = pd.DataFrame(fourier_terms, index=all_data.index)\n",
        "fourier_df.columns = [f'fourier_cos_{i+1}' for i in range(len(periods))] + [f'fourier_sin_{i+1}' for i in range(len(periods))]\n",
        "all_data = pd.concat([all_data, fourier_df], axis=1)\n",
        "\n",
        "all_data.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qb07uxZns0EA"
      },
      "outputs": [],
      "source": [
        "# Feature engineering: Including lagged features for indicators\n",
        "lags = 5\n",
        "features_to_use = ['Log_Returns', 'RSI', 'MACD', 'SMA', 'EMA', 'ATR', 'UpperBB', 'MiddleBB', 'LowerBB',\n",
        "                   'day_of_week', 'month', 'quarter', 'rolling_mean_30', 'rolling_mean_90',\n",
        "                   'rolling_std_30'] + list(fourier_df.columns)\n",
        "\n",
        "# Include lagged features for all\n",
        "for feature in features_to_use:\n",
        "    for lag in range(1, lags + 1):\n",
        "        all_data[f'{feature}_lag_{lag}'] = all_data[feature].shift(lag)\n",
        "\n",
        "all_data.dropna(inplace=True)\n",
        "\n",
        "print(all_data.head)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBOmSxUVx689"
      },
      "outputs": [],
      "source": [
        "assert not all_data.isnull().values.any(), \"NaN values are present in the dataset.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYyOcGx4bHuM"
      },
      "outputs": [],
      "source": [
        "def handle_outliers_with_count(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    outliers_before = ((df[column] < lower_bound) | (df[column] > upper_bound)).sum()\n",
        "\n",
        "    df[column] = np.where(df[column] < lower_bound, lower_bound, df[column])\n",
        "    df[column] = np.where(df[column] > upper_bound, upper_bound, df[column])\n",
        "\n",
        "    outliers_after = ((df[column] < lower_bound) | (df[column] > upper_bound)).sum()\n",
        "\n",
        "    return df, outliers_before, outliers_after\n",
        "\n",
        "features = ['RSI', 'MACD', 'SMA', 'EMA', 'ATR', 'UpperBB', 'MiddleBB', 'LowerBB', 'Log_Returns']\n",
        "for feature in features:\n",
        "    all_data, outliers_before, outliers_after = handle_outliers_with_count(all_data, feature)\n",
        "    print(f\"Feature '{feature}': Outliers before = {outliers_before}, Outliers after = {outliers_after}\")\n",
        "\n",
        "print(\"Data preprocessing is complete. The dataset is ready for scaling and LSTM modeling.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnBtQ_IbbsS1"
      },
      "outputs": [],
      "source": [
        "# Define the features (X) and the target (y)\n",
        "feature_columns = [col for col in all_data.columns if 'lag' in col]\n",
        "X = all_data[feature_columns]\n",
        "y = all_data['Log_Returns']\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Define the sequence length\n",
        "sequence_length = 10\n",
        "\n",
        "def create_sequences(data, seq_length):\n",
        "    xs = []\n",
        "    ys = []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        x = data[i:(i + seq_length)]\n",
        "        y = data[i + seq_length][-1]\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "    return np.array(xs), np.array(ys)\n",
        "\n",
        "# Recreate sequences with the adjusted function\n",
        "X_seq, y_seq = create_sequences(X_scaled, sequence_length)\n",
        "\n",
        "# After adjusting, ensure the shapes align\n",
        "print(\"Shape of X_seq:\", X_seq.shape)\n",
        "print(\"Shape of y_seq:\", y_seq.shape)\n",
        "\n",
        "# Split the data into training and test sets without shuffling\n",
        "train_proportion = 0.9\n",
        "split_idx = int(len(X_seq) * train_proportion)\n",
        "X_train_seq, y_train_seq = X_seq[:split_idx], y_seq[:split_idx]\n",
        "X_test_seq, y_test_seq = X_seq[split_idx:], y_seq[split_idx:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TPStAWMwo9IG"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model = Sequential([\n",
        "    Bidirectional(LSTM(32, activation='tanh', input_shape=(sequence_length, X_train_seq.shape[2]), return_sequences=True,\n",
        "                       kernel_regularizer=l2(0.01))),\n",
        "    Dropout(0.2),\n",
        "    Bidirectional(LSTM(32, activation='tanh', kernel_regularizer=l2(0.01))),\n",
        "    Dropout(0.2),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_seq, y_train_seq,\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "test_loss = model.evaluate(X_test_seq, y_test_seq, verbose=1)\n",
        "print(f'Test Loss: {test_loss}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "anXQHjDl9tWo"
      },
      "outputs": [],
      "source": [
        "predicted_in_sample = model.predict(X_train_seq).flatten()\n",
        "\n",
        "residuals_in_sample = y_train_seq.flatten() - predicted_in_sample\n",
        "\n",
        "predicted_out_of_sample = model.predict(X_test_seq).flatten()\n",
        "\n",
        "residuals_out_of_sample = y_test_seq.flatten() - predicted_out_of_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Uf8InsIYNAEi"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "mae_lstm = mean_absolute_error(y_test_seq, predicted_out_of_sample)\n",
        "rmse_lstm = np.sqrt(mean_squared_error(y_test_seq, predicted_out_of_sample))\n",
        "\n",
        "print(f'LSTM MAE: {mae_lstm}')\n",
        "print(f'LSTM RMSE: {rmse_lstm}')\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Learning Curves (Loss over Epochs)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(y_test_seq, label='Actual Returns')\n",
        "plt.plot(predicted_out_of_sample, label='Predicted Returns')\n",
        "plt.title('Actual vs Predicted Returns')\n",
        "plt.xlabel('Time Steps')\n",
        "plt.ylabel('Returns')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(residuals_out_of_sample, label='Residuals')\n",
        "plt.hlines(y=0, xmin=0, xmax=len(residuals_out_of_sample), colors='r', linestyles='--')\n",
        "plt.title('Residuals of Predictions')\n",
        "plt.xlabel('Time Steps')\n",
        "plt.ylabel('Residual')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(residuals_out_of_sample, bins=50, label='Residuals', color='blue', edgecolor='black')\n",
        "plt.title('Histogram of Residuals')\n",
        "plt.xlabel('Residual')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0sVLMFeW9wSJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from arch import arch_model\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from arch import arch_model\n",
        "from arch.univariate import FIGARCH\n",
        "\n",
        "# Define additional functions for metrics\n",
        "def mape(actual, predicted):\n",
        "    return np.mean(np.abs((actual - predicted) / actual)) * 100\n",
        "\n",
        "def theils_u(actual, predicted, naive_forecast):\n",
        "    actual_diff = actual[1:] - naive_forecast[:-1]\n",
        "    predicted_diff = predicted[1:] - naive_forecast[:-1]\n",
        "    return np.sqrt(np.mean(np.square(predicted_diff / actual_diff)))\n",
        "\n",
        "# Define the model space for hyperparameter tuning\n",
        "p_values = [0, 1]  # p parameter\n",
        "q_values = [0, 1]  # q parameter\n",
        "power_values = [1.0, 2.0]  # Power parameter, for FIAVGARCH or FIGARCH\n",
        "\n",
        "# Placeholder for storing results\n",
        "results = []\n",
        "\n",
        "for p in p_values:\n",
        "    for q in q_values:\n",
        "        for power in power_values:\n",
        "            try:\n",
        "                model = arch_model(residuals_in_sample, vol='Figarch', p=p, q=q, power=power)\n",
        "                res = model.fit(disp='off')\n",
        "\n",
        "                 # Store the results\n",
        "                results.append({\n",
        "                    'p': p,\n",
        "                    'q': q,\n",
        "                    'power': power,\n",
        "                    'AIC': res.aic\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred: {e}. Params: p={p}, q={q}, power={power}\")\n",
        "\n",
        "# Convert results to DataFrame for easier handling\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Find the best model based on AIC\n",
        "best_model_info = results_df.loc[results_df['AIC'].idxmin()]\n",
        "print(\"Best FIGARCH model configuration based on AIC:\")\n",
        "print(best_model_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMOGSyKIRcPh"
      },
      "outputs": [],
      "source": [
        "# Best model configuration (Update these based on your best model found)\n",
        "best_p = int(best_model_info['p'])\n",
        "best_q = int(best_model_info['q'])\n",
        "best_power = best_model_info['power']\n",
        "\n",
        "# Fitting FIGARCH model (this syntax is hypothetical and depends on library support)\n",
        "model = arch_model(residuals_in_sample, mean='Zero', vol='Figarch', p=best_p, q=best_q, power=best_power)\n",
        "fitted_model = model.fit(disp='off')\n",
        "\n",
        "# Forecasting with the fitted model\n",
        "predicted_variances = []\n",
        "for i in range(len(residuals_out_of_sample)):\n",
        "    forecasts = fitted_model.forecast(horizon=1, start=i, method='simulation')\n",
        "    predicted_variance = forecasts.variance.values[-1, :]\n",
        "    predicted_variances.append(predicted_variance)\n",
        "\n",
        "predicted_variances = np.array(predicted_variances[:len(residuals_out_of_sample)])\n",
        "\n",
        "# Calculate MAE and RMSE\n",
        "actual_variances = np.square(residuals_out_of_sample)\n",
        "mae = mean_absolute_error(actual_variances, predicted_variances)\n",
        "rmse = np.sqrt(mean_squared_error(actual_variances, predicted_variances))\n",
        "\n",
        "print(f'MAE: {mae}, RMSE: {rmse}')\n",
        "\n",
        "# Plotting results\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(actual_variances, label='Actual Variance')\n",
        "plt.plot(predicted_variances, label='Predicted Variance', alpha=0.7)\n",
        "plt.title('Actual vs Predicted Variance')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0a1rAC3sb4F"
      },
      "outputs": [],
      "source": [
        "# Sharpe Ratio Function\n",
        "def sharpe_ratio(return_series, risk_free_rate=0.0):\n",
        "    # Convert annual risk-free rate to per period\n",
        "    rf_per_period = (1 + risk_free_rate)**(1/252) - 1\n",
        "    excess_ret = return_series - rf_per_period\n",
        "    return np.mean(excess_ret) / np.std(excess_ret)\n",
        "\n",
        "# Calculate Sharpe Ratio for LSTM predicted returns\n",
        "sharpe_ratio_lstm = sharpe_ratio(predicted_out_of_sample)\n",
        "\n",
        "# Histogram of Out-of-Sample Residuals\n",
        "plt.hist(residuals_out_of_sample, bins=50)\n",
        "plt.title('Histogram of Out-of-Sample Residuals')\n",
        "plt.show()\n",
        "\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Q-Q Plot Function\n",
        "def qq_plot(residuals):\n",
        "    stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
        "    plt.title('Q-Q Plot of Out-of-Sample Residuals')\n",
        "    plt.show()\n",
        "\n",
        "# Create Q-Q Plot\n",
        "qq_plot(residuals_out_of_sample)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}